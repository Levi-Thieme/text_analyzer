package bigData;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileReader;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.time.Instant;
import java.util.HashMap;
import java.util.StringTokenizer;
import java.util.TreeMap;
import java.util.concurrent.ConcurrentMap;
import java.util.stream.Stream;

import org.mapdb.BTreeMap;
import org.mapdb.DB;
import org.mapdb.DBMaker;
import org.mapdb.HTreeMap;
import org.mapdb.Serializer;

public class Driver {

	public static void main(String[] args) {
		DB db = DBMaker
		        .fileDB("file.db")
		        .fileMmapEnable()
		        .make();
		HTreeMap<String, Long> map = db
		        .hashMap("map", Serializer.STRING, Serializer.LONG)
		        .createOrOpen();
		
		Long start = Instant.now().toEpochMilli();
		try (Stream<String> stream = Files.lines(Paths.get("data/bookCopies.txt"))) {
			stream.forEach(line -> {
				map.put(line, 1L);
				/*
				StringTokenizer tokenizer = new StringTokenizer(line);
				while(tokenizer.hasMoreTokens()) {
					String word = tokenizer.nextToken();
					map.put(word, map.get(word) == null ? 1L : map.get(word) + 1L);
				}
				*/
			});
		} catch (IOException e) {
			e.printStackTrace();
		}
		System.out.println("Runtime: " + (Instant.now().toEpochMilli() - start) / 1000);
		System.out.println(map.size() + " " + map.getEntries().iterator().next().toString());
		db.close();
	}

	
}
